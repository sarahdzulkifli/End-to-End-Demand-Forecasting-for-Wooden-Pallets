{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Administrator\\\\Demand Forecasting 24122024\\\\End-to-End-Demand-Forecasting-for-Wooden-Pallets\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Administrator\\\\Demand Forecasting 24122024\\\\End-to-End-Demand-Forecasting-for-Wooden-Pallets'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DemandForecasting_WoodenPallets.constant import *\n",
    "from DemandForecasting_WoodenPallets.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH,\n",
    "        schema_filepath = SCHEMA_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "\n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        config = self.config.data_transformation\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_transformation_config = DataTransformationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            data_path=config.data_path,\n",
    "        )\n",
    "\n",
    "        return data_transformation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from DemandForecasting_WoodenPallets import logger\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransformation:\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def train_test_splitting(self):\n",
    "        df = pd.read_excel(self.config.data_path)\n",
    "\n",
    "        # EDA: Basic data cleaning and exploration\n",
    "        df = df.drop_duplicates()\n",
    "        df = df.drop(\"NumAtCard\", axis=1)  # Drop 100% missing data for \"NumAtCard\" column\n",
    "        df['POSTING DATE'] = pd.to_datetime(df['POSTING DATE'], errors='coerce')\n",
    "        df['EFFECTIVE DATE'] = pd.to_datetime(df['EFFECTIVE DATE'], errors='coerce')\n",
    "        df['CREATE DATE'] = pd.to_datetime(df['CREATE DATE'], errors='coerce')\n",
    "        df['SO Creation Date'] = pd.to_datetime(df['SO Creation Date'])\n",
    "        df['SO Due Date'] = pd.to_datetime(df['SO Due Date'])\n",
    "        df['QUANTITY'] = df['QUANTITY'].astype(int)\n",
    "        df['RATE'] = df['RATE'].astype(float)\n",
    "\n",
    "        # Convert to 'category' type for categorical columns\n",
    "        df['LOB'] = df['LOB'].astype('category')\n",
    "        df['Region'] = df['Region'].astype('category')\n",
    "        df['BP TYPE'] = df['BP TYPE'].astype('category')\n",
    "        df['PRODUCT CATEGORY'] = df['PRODUCT CATEGORY'].astype('category')\n",
    "        df['Customer/Vendor Code'] = df['Customer/Vendor Code'].astype(str)\n",
    "\n",
    "        # Cleaning city and state columns\n",
    "        df['City'] = df['City'].str.title()\n",
    "        city_std = {'Vijaywada': 'Vijayawada'}\n",
    "        state_std = {'Chattisgarh': 'Chhattisgarh'}\n",
    "\n",
    "        df['City'] = df['City'].replace(city_std)\n",
    "        df['STATE'] = df['STATE'].replace(state_std)\n",
    "\n",
    "        # Extract features from the date columns\n",
    "        df['posting_day_of_week'] = df['POSTING DATE'].dt.day_name()\n",
    "        df['posting_month'] = df['POSTING DATE'].dt.month\n",
    "        df['posting_year'] = df['POSTING DATE'].dt.year\n",
    "        df['so_creation_year'] = df['SO Creation Date'].dt.year\n",
    "        df['so_creation_month'] = df['SO Creation Date'].dt.month\n",
    "        df['so_due_year'] = df['SO Due Date'].dt.year\n",
    "        df['so_due_month'] = df['SO Due Date'].dt.month\n",
    "\n",
    "        # Calculate the lead time (processing time)\n",
    "        df['effective_to_posting'] = (df['EFFECTIVE DATE'] - df['POSTING DATE']).dt.days\n",
    "        df['lead_time'] = (df['SO Due Date'] - df['SO Creation Date']).dt.days\n",
    "\n",
    "        df['posting_quarter'] = df['POSTING DATE'].dt.quarter\n",
    "\n",
    "        # Date falls on weekend\n",
    "        df['is_posting_weekend'] = df['POSTING DATE'].dt.dayofweek >= 5\n",
    "\n",
    "        # Inventory activity at month-end\n",
    "        df['is_posting_month_end'] = df['POSTING DATE'].dt.is_month_end\n",
    "\n",
    "        # Now split data into training and test sets (0.75, 0.25) split.\n",
    "        train, test = train_test_split(df, test_size=0.25, random_state=42)\n",
    "\n",
    "        # Continue with encoding categorical columns\n",
    "        train_encoded, test_encoded = self.encode_categorical_columns(train, test)\n",
    "\n",
    "        # Save the encoded data\n",
    "        train_encoded.to_excel(os.path.join(self.config.root_dir, \"train_encoded.xlsx\"), index=False)\n",
    "        test_encoded.to_excel(os.path.join(self.config.root_dir, \"test_encoded.xlsx\"), index=False)\n",
    "\n",
    "        logger.info(\"Splitted data into training and test sets\")\n",
    "        logger.info(f\"Train shape: {train_encoded.shape}\")\n",
    "        logger.info(f\"Test shape: {test_encoded.shape}\")\n",
    "\n",
    "        print(train_encoded.shape)\n",
    "        print(test_encoded.shape)\n",
    "\n",
    "    def encode_categorical_columns(self, train_df, test_df):\n",
    "        categorical_columns = train_df.select_dtypes(include=['object']).columns\n",
    "        le = LabelEncoder()\n",
    "\n",
    "    # Create a new DataFrame to hold the transformed data\n",
    "        train_encoded = train_df.copy()\n",
    "        test_encoded = test_df.copy()\n",
    "\n",
    "        for col in categorical_columns:\n",
    "            if train_df[col].nunique() < 10:  # Use one-hot for small unique categories\n",
    "                # Fit on training data and transform both train and test\n",
    "                dummies_train = pd.get_dummies(train_df[col], prefix=col, drop_first=True)\n",
    "                dummies_test = pd.get_dummies(test_df[col], prefix=col, drop_first=True)\n",
    "\n",
    "                # Align columns to ensure both DataFrames have the same columns\n",
    "                dummies_train, dummies_test = dummies_train.align(dummies_test, join='outer', axis=1, fill_value=0)\n",
    "\n",
    "                train_encoded = pd.concat([train_encoded, dummies_train], axis=1)\n",
    "                test_encoded = pd.concat([test_encoded, dummies_test], axis=1)\n",
    "\n",
    "                train_encoded.drop(col, axis=1, inplace=True)\n",
    "                test_encoded.drop(col, axis=1, inplace=True)\n",
    "            elif train_df[col].nunique() < 50:  # Use label encoding for mid-range categories\n",
    "                # Fit the encoder on the training data\n",
    "                train_encoded[col] = le.fit_transform(train_df[col].astype(str))\n",
    "                # Transform the test data, handling unseen labels\n",
    "                test_encoded[col] = test_df[col].astype(str).map(lambda x: le.transform([x])[0] if x in le.classes_ else -1)  # Map unseen labels to -1\n",
    "            else:  # Use frequency encoding for high unique categories\n",
    "                freq_map = train_df[col].value_counts().to_dict()\n",
    "                train_encoded[col] = train_df[col].map(freq_map)\n",
    "                test_encoded[col] = test_df[col].map(freq_map).fillna(0)  # Fill NaN with 0 or another value\n",
    "\n",
    "        return train_encoded, test_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-03-30 09:29:34,099: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2025-03-30 09:29:34,103: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-03-30 09:29:34,107: INFO: common: yaml file: schema.yaml loaded successfully]\n",
      "[2025-03-30 09:29:34,110: INFO: common: created directory at: artifacts]\n",
      "[2025-03-30 09:29:34,110: INFO: common: created directory at: artifacts/data_transformation]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\anaconda3\\envs\\MLProj\\lib\\site-packages\\openpyxl\\worksheet\\_read_only.py:85: UserWarning: Unknown extension is not supported and will be removed\n",
      "  for idx, row in parser.parse():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-03-30 09:29:38,208: INFO: 1414003474: Splitted data into training and test sets]\n",
      "[2025-03-30 09:29:38,208: INFO: 1414003474: Train shape: (1032, 49)]\n",
      "[2025-03-30 09:29:38,211: INFO: 1414003474: Test shape: (344, 49)]\n",
      "(1032, 49)\n",
      "(344, 49)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_transformation_config = config.get_data_transformation_config()\n",
    "    data_transformation = DataTransformation(config=data_transformation_config)\n",
    "    data_transformation.train_test_splitting()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLProj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
